{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo we will use Amazon Comprehend to create a custom document classifier.\n",
    "\n",
    "---\n",
    "Load the needed the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import feedparser\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "import io\n",
    "import gzip\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "comprehend = boto3.client('comprehend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Update the following two values\n",
    "\n",
    "1. `bucket` - Enter S3 bucket which will be used to stage the files.\n",
    "2. `object_prefix` - Enter the prefix in S3 where the object should be uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'ai-data-eu-west-1'\n",
    "object_prefix = 'comprehend'\n",
    "\n",
    "test_data_file = 'ag_news_csv/testdata.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We are using [AG's corpus of news articles](http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) data set to train our Comprehend Custom Model.\n",
    "\n",
    "Load the data set in CSV format as pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('ag_news_csv/aggregates.csv', engine='python', names=['ID','Title','Description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Drop the *Title* column since we need only class name (ID) and Description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop('Title', axis=1)\n",
    "train_data.sample(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Write the data frame to a CSV file and upload it to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"ag_news_csv/processed.csv\", index= False, header = False)\n",
    "s3.Bucket(bucket).upload_file(\n",
    "    'ag_news_csv/processed.csv',\n",
    "    object_prefix + '/news-classification.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a document classifier job in Comprehend by providing the S3 location where we uploaded the file.\n",
    "\n",
    "You also need an IAM role which has permission to access the S3 location as part of permission policy and the trust policy should allow Comprehend to assume the role.\n",
    "\n",
    "Update the variable `data_acess_iam_role` and enter the ARN of the IAM role with the above mentioned policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y-%m-%d-%H%M%S')\n",
    "input_data = 's3://{}/{}/news-classification.csv'.format(bucket, object_prefix)\n",
    "data_acess_iam_role = 'arn:aws:iam::0123345678901:role/service-role/AmazonComprehendServiceRoleS3Access'\n",
    "\n",
    "classifier = comprehend.create_document_classifier(\n",
    "    DocumentClassifierName='docclassifier-techsummit-demo-' + timestamp,\n",
    "    DataAccessRoleArn=data_acess_iam_role,\n",
    "    InputDataConfig={\n",
    "        'S3Uri': input_data\n",
    "    },\n",
    "    LanguageCode='en'\n",
    ")\n",
    "classifierArn = classifier['DocumentClassifierArn']\n",
    "print(classifierArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's describe the document classifier job to validate it. Status should be either 'SUBMITTED' or 'TRAINING'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_status = comprehend.describe_document_classifier(\n",
    "    DocumentClassifierArn=classifierArn\n",
    ")\n",
    "print(json.dumps(classifier_status, sort_keys=True, indent=4, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now let's prepare our test data set. We are going to use the BBC RSS feeds of categories World, Business, Technology and Sports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_feed = 'http://feeds.bbci.co.uk/news/world/rss.xml'\n",
    "business_feed = 'http://feeds.bbci.co.uk/news/business/rss.xml'\n",
    "tech_feed = 'http://feeds.bbci.co.uk/news/technology/rss.xml'\n",
    "sports_feed = 'http://feeds.bbci.co.uk/sport/rss.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_items = feedparser.parse(world_feed)['entries']\n",
    "business_items = feedparser.parse(business_feed)['entries']\n",
    "tech_items = feedparser.parse(tech_feed)['entries']\n",
    "sports_items = feedparser.parse(sports_feed)['entries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Get the top 4 articles from the feeds and extract the summary of the news article and store it to a file.\n",
    "\n",
    "Upload that file to S3 which shall be used to test the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_data_file, 'w+') as testfile:\n",
    "    for feed in [world_items, business_items, tech_items, sports_items]:\n",
    "        for i in range(0,4):\n",
    "            testfile.write(feed[i]['summary'] + '\\n')\n",
    "            \n",
    "s3.Bucket(bucket).upload_file(test_data_file, object_prefix + '/testdata.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's make sure that the training job has completed successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Document Classifier job is in progress ', end='')\n",
    "while True:\n",
    "    status = comprehend.describe_document_classifier(DocumentClassifierArn=classifierArn\n",
    ")\n",
    "    if status['DocumentClassifierProperties']['Status'] in ['TRAINED', 'FAILED']:\n",
    "        print('')\n",
    "        print('Document Classifier job completed with status %s\\n' % status['DocumentClassifierProperties']['Status'])\n",
    "        break\n",
    "    print('.', end='')\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now we have a trained classifier. Createa document classification job and provide the test data set as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y-%m-%d-%H%M%S')\n",
    "classify_job = comprehend.start_document_classification_job(\n",
    "    JobName='classify-techsummit-demo-' + timestamp,\n",
    "    DocumentClassifierArn=classifierArn,\n",
    "    InputDataConfig={\n",
    "        'S3Uri': 's3://{}/{}/testdata.txt'.format(bucket, object_prefix),\n",
    "        'InputFormat': 'ONE_DOC_PER_LINE'\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': 's3://{}/{}/output/custom-classifier/'.format(bucket, object_prefix)\n",
    "    },\n",
    "    DataAccessRoleArn=data_acess_iam_role\n",
    ")\n",
    "print('Job with Id %s is in status %s' % (classify_job['JobId'], classify_job['JobStatus']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Wait for the classification job to be complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Custom Classification job is in progress ', end='')\n",
    "while True:\n",
    "    status = comprehend.describe_document_classification_job(JobId=classify_job['JobId'])\n",
    "    if status['DocumentClassificationJobProperties']['JobStatus'] in ['COMPLETED', 'FAILED']:\n",
    "        print('')\n",
    "        print('Custom Classification completed with status %s\\n' % status['DocumentClassificationJobProperties']['JobStatus'])\n",
    "        break\n",
    "    print('.', end='')\n",
    "    time.sleep(5)\n",
    "\n",
    "o_bucket = status['DocumentClassificationJobProperties']['OutputDataConfig']['S3Uri'].split('/')[2]\n",
    "o_key = '/'.join(status['DocumentClassificationJobProperties']['OutputDataConfig']['S3Uri'].split('/')[3:])\n",
    "\n",
    "output_gzip = gzip.open(BytesIO(s3.Object(o_bucket, o_key).get()['Body'].read()), 'rt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now let's look at the result of classification job for our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = True\n",
    "line_count = 0\n",
    "classify_result = []\n",
    "for line in output_gzip.readlines():\n",
    "    if flag:\n",
    "        line = '{' + line.split(sep='{', maxsplit=1)[1]\n",
    "        flag = False\n",
    "    classify_result.append(json.loads(line))\n",
    "    line_count += 1\n",
    "    if line_count == 16:\n",
    "        break\n",
    "        \n",
    "class_list = {'1': 'World', '2': 'Sports', '3': 'Business', '4': 'Science_Technology'}\n",
    "\n",
    "with open(test_data_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for res in classify_result:\n",
    "        line_num = int(res['Line'])\n",
    "        class_detected = res['Classes'][0]['Name']\n",
    "        class_score = res['Classes'][0]['Score']\n",
    "        print('Article with description \"%s\" is classified as \"%s\" with score \"%s\".' % \n",
    "              (lines[line_num].strip(), class_list[class_detected], class_score))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
