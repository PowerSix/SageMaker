{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all of the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import boto3\n",
    "import json\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from imageio import imread\n",
    "import pandas as pd\n",
    "\n",
    "import base64\n",
    "\n",
    "#Implement AWS Services\n",
    "rekognition=boto3.client('rekognition')\n",
    "comprehendmedical = boto3.client(service_name='comprehendmedical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Load the chest x-ray image as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xray_file='chest-xray.png'\n",
    "redacted_box_color='red'\n",
    "dpi = 72\n",
    "phi_detection_threshold = 0.00\n",
    "\n",
    "img = np.array(Image.open(xray_file), dtype=np.uint8)\n",
    "\n",
    "#Set the image color map to grayscale, turn off axis graphing, and display the image\n",
    "height, width = img.shape\n",
    "# What size does the figure need to be in inches to fit the image?\n",
    "figsize = width / float(dpi), height / float(dpi)\n",
    "# Create a figure of the right size with one axes that takes up the full figure\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "# Hide spines, ticks, etc.\n",
    "ax.axis('off')\n",
    "# Display the image.\n",
    "ax.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Use Amazon Rekognition to detect the text in the image along with the coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Amazon Rekognition to detect all of the text in the medical image\n",
    "with open(xray_file, 'rb') as image:\n",
    "    response=rekognition.detect_text(Image={'Bytes': image.read()})\n",
    "\n",
    "textDetections=response['TextDetections']\n",
    "print('Aggregating detected text...')\n",
    "textblock=\"\"\n",
    "offsetarray=[]\n",
    "totallength=0\n",
    "\n",
    "# The various text detections are returned in a JSON object.  Aggregate the text into a single large block and\n",
    "# keep track of the offsets.  This will allow us to make a single call to Amazon Comprehend Medical for\n",
    "# PHI detection and minimize our Comprehend Medical service charges.\n",
    "for text in textDetections:\n",
    "    if text['Type'] == \"LINE\":\n",
    "            offsetarray.append(totallength)\n",
    "            totallength+=len(text['DetectedText'])+1\n",
    "            textblock=textblock+text['DetectedText']+\" \"  \n",
    "            print(text['DetectedText']+\"', length: \"+str(len(text['DetectedText']))+\", offsetarray: \"+str(offsetarray))\n",
    "offsetarray.append(totallength)\n",
    "totaloffsets=len(offsetarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Using Comprehend Medical, detect PHI from the text detected in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Amazon Comprehend Medical and pass it the aggregated text from our medical image.\n",
    "phi_boxes_list=[]\n",
    "philist=comprehendmedical.detect_phi(Text = textblock)\n",
    "\n",
    "# Amazon Comprehend Medical will return a JSON object that contains all of the PHI detected in the text block with\n",
    "# offset values that describe where the PHI begins and ends.  We can use this to determine which of the text blocks \n",
    "# detected by Amazon Rekognition should be redacted.  The 'phi_boxes_list' list is created to keep track of the\n",
    "# bounding boxes that potentially contain PHI.\n",
    "print('Finding PHI text...')\n",
    "not_redacted=0\n",
    "for phi in philist['Entities']:\n",
    "    if phi['Score'] > phi_detection_threshold:\n",
    "        for i in range(0,totaloffsets-1):\n",
    "            if offsetarray[i] <= phi['BeginOffset'] < offsetarray[i+1]:\n",
    "                if textDetections[i]['Geometry']['BoundingBox'] not in phi_boxes_list:\n",
    "                    print(\"'\"+phi['Text']+\"' was detected as type '\"+phi['Type']+\"' and will be redacted.\")\n",
    "                    phi_boxes_list.append(textDetections[i]['Geometry']['BoundingBox'])\n",
    "    else:\n",
    "        print(\"'{}' was detected as type '{}', but did not meet the confidence score threshold and will not be redacted.\" % (phi['Text'], phi['Type']))\n",
    "        not_redacted+=1\n",
    "print(\"Found %d text boxes to redact.\" % (len(phi_boxes_list)))\n",
    "print(\"%d additional text boxes were detected, but did not meet the confidence score threshold.\" % (not_redacted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Based on the detected PHI, mask the section of the image with red boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now this list of bounding boxes will be used to draw red boxes over the PHI text.\n",
    "height, width = img.shape\n",
    "# What size does the figure need to be in inches to fit the image?\n",
    "figsize = width / float(dpi), height / float(dpi)\n",
    "# Create a figure of the right size with one axes that takes up the full figure\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.imshow(img)\n",
    "plt.imshow(img, cmap='gray')\n",
    "for box in phi_boxes_list:\n",
    "    #The bounding boxes are described as a ratio of the overall image dimensions, so we must multiply them\n",
    "    #by the total image dimensions to get the exact pixel values for each dimension.\n",
    "    x = img.shape[0] * box['Left']\n",
    "    y = img.shape[1] * box['Top']\n",
    "    width = img.shape[0] * box['Width']\n",
    "    height = img.shape[1] * box['Height']\n",
    "    rect = patches.Rectangle((x,y),width,height,linewidth=0,edgecolor=redacted_box_color,facecolor=redacted_box_color)\n",
    "    ax.add_patch(rect)\n",
    "#Ensure that no axis or whitespaces is printed in the image file we want to save.\n",
    "plt.axis('off')    \n",
    "plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "#Save redacted medical image to the same Amazon S3 bucket, in PNG format, with 'de-id-' in front of the original\n",
    "#filename.\n",
    "img_data = io.BytesIO()\n",
    "plt.savefig(img_data, bbox_inches='tight', pad_inches=0, format='png')\n",
    "img_data.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
